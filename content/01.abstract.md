## Abstract {.page_break_before}

Reasoning is a core component of human intelligence, and a key goal of Artificial Intelligence (AI) research.
Reasoning has traditionally been the domain of symbolic AI, but recent advances in deep learning
and in particular Large Language Models (LLMs) such as GPT-4 indicate that LLMs may have
some latent reasoning ability. This has been investigated in some domains such as mathematical reasoning,
but the ability of LLMs to reason over knowledge representation formalisms such as Description Logics (DL) has not been
systematically tested. Understanding the accuracy of this latent reasoning behavior is important for
designing LLM-based applications, in particular in deciding between single-pass approaches vs hybrid
or multi-agent approaches.

To investigate the latent abilities of LLMs to perform symbolic ontological reasoning, we
created a series of benchmarks covering the EL++ fragment of the Web Ontology Language (OWL).
We also created a GPT-based reasoning agent that uses a few-shot in-context learning approach to reason
over OWL ontologies.

Our results show that for small ontologies and certain simple tasks such as transitive SubClassOf
reasoning, or inferring familial relationships, GPT-4 frequently yields the correct answers to 
benchmark questions, although less than the 100% guaranteed accuracy of a symbolic reasoner. For other
tasks, LLMs performed less well, and in particular LLMs seem fundamentally unable to detect
inconsistencies in even simple small test cases.

Our results indicate that a maximalist single-pass approach to building LLMs knowledge-based applications inherently
leads to lower accuracy, and that a hybrid multi-agent approach combining the best aspects of
language models and symbolic reasoning over knowledge bases should be favored.

Code is available from https://github.com/monarch-initiative/ontogpt, and analysis from TODO zenodo DOI

