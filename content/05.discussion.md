## Discussion

### Implications of study

In this study we set out to investigate the performance of LLMs on ontological reasoning tasks.
We consider the question of whether LLMs are actually performing reasoning to be out of scope.
It may be the case that the LLMs are simply relying on higher-order pattern matching against a
massive bank of examples.

We are concerned with performance, because we observe that there is a tendency to use LLMs
as single-pass question-answer agents, where reasoning may be a subcomponent of the overall
task. Sometimes the reasoning step might not be explicit, and the effects of incomplete
reasoning may not be readily apparent. For example, when performing a task such as gene set
summarization, we would like the LLM to take into account hierarchical classification of gene
descriptors, and to make use of basic reasoning as part of finding commonalities.
If it is the case that LLMs are incomplete reasoners on reasoning-specific tasks, it is likely
they will yield incomplete results on tasks that involve a reasoning component.
This would suggest that hybrid strategies that combine LLMs with more traditional reasoning
would yield the best results.

There are a number of different mechanisms by which an LLM-based architecture could leverage
a hybrid approach. The ChatGPT plugin system allows an LLM to be combined with crisp knowledge
base lookup and sound inference procedures - for example, via the Mathematica plugin. Frameworks
such as LangChain allow for use of the ReAct and MKLR patterns, in which the LLM can act as a
controller, and select an external service or agent for a component of task. Currently there
are no symbolic reasoner plugins of the form used for ontological reasoning, but these
could form a powerful part of a hybrid neurosymbolic system.

### Benchmark limitations

Our benchmark currently only covers the EL++ subset of OWL (the taxon constraint task is
explicitly formulated to include non-EL axioms, but the task can easily be reformulated
as an EL++ one).

The benchmarks are intentionally small, due to the need to fit in current LLM content windows
(8k for the standard GPT-4 model). Most ontologies are typically larger, so even in cases
where LLMs perform well, it doesn't necessarily hold that an LLM with a larger context window
will perform as well for larger ontologies.


