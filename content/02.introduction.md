## Introduction

### Motivation

The field of Artificial Intelligence (AI) has historically been dichotomized into two schools of thought:
symbolic AI and connectionist AI. Symbolic AI is exemplified by ontologies and knowledge bases which represent
knowledge as a set of symbols together with rules for manipulating those symbols. Connectionist AI is exemplified by
deep learning and in particular Large Language Models (LLMs).

Recent advances in LLMs have shown exceptional abilities in tasks such as question-answering and text summarization,
with the use of in-context learning able to substitute for task-specific training[@doi:10.48550/arXiv.2005.14165].
The abilities of the latest generation of LLMs such as GPT-4 seem to suggest that LLMs may be able to perform
reasoning tasks that were previously the domain of symbolic AI, in the same way that a human may incorporate
reasoning as a part of natural language question-answering. The implication here is that symbolic AI and
classical deductive reasoning, if this is an emergent ability of LLMs. However, the ability of LLMs to
reason over knowledge bases has never been systematically evaluated.

### Ontological Reasoning

Ontology reasoning is a form of reasoning that is based on the structure of a knowledge base or ontology.

Ontology reasoning underpins...

### OWL Benchmark Datasets

- OWL2Bench
- LUBM

### Natural Language Benchmark Datasets

Existing datasets for testing reasoning ability include  LogiQA [@doi:10.48550/arXiv.2007.08124]
and ReClor [@doi:10.48550/arXiv.2002.04326]. These are aimed at testing reasoning abilities in the
context of natural language understanding. For example, LogiQA contains as instance with paragraph
"_David knows Mr. Zhang's friend Jack, and Jack knows David's friend  Ms. Lin. Everyone of them who knows Jack has a master's degree, and  everyone of them who knows Ms. Lin is from Shanghai._" and question "_Who is from Shanghai and has a masters degree?_".
Liu et al adapted these for evaluation using prompt-based LLMs such as GPT [@doi:10.48550/arXiv.2304.03439]..

### Contributions

We make the following contributions:

- We have curated a collection of benchmarks for testing ontological reasoning ability
- We created a GPT-based reasoning agent that is intended to perform ontological reason using
  a few-shot learning approach, using instruction prompting and in-context examples.
- We have evaluated the reasoning ability of GPT-3.5-turbo and GPT-4 on these benchmarks