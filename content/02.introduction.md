## Introduction

### Motivation

The field of Artificial Intelligence (AI) has historically been divided into two schools of thought:
symbolic AI and connectionist AI. Symbolic AI is exemplified by ontologies and knowledge bases, which represent
knowledge as a set of symbols together with rules or formal model theories for manipulating those symbols.
Connectionist AI is exemplified by methods derived from statistics and machine learning, in more recently deep learning neural network
architectures. The field of neurosymbolic AI aims to combine the best aspects of both symbolic and connectionist AI,
but currently most approaches fall in one of the two camps.

The most striking recent development in machine learning has been the development of Large Language Models (LLMs),
which show exceptional abilities in tasks such as question-answering and text summarization,
as well as the ability to generalize from a handful of examples (in context or few-shot learning)
[@doi:10.48550/arXiv.2005.14165]. 
The abilities of the latest generation of LLMs such as GPT-4 seem to suggest that LLMs may be able to perform
reasoning tasks that were previously the domain of symbolic AI, in the same way that a human may incorporate
reasoning as a part of natural language question-answering. If LLMs are in fact able to effectively perform symbolic reasoning as part of their normal prompt completion process, this would suggest less of a role for symbolic AI and
explicit rule-based reasoning. If instead LLMs are only able to perform reasoning in a limited set of circumstances,
this would suggest that LLM-based applications should employ hybrid or multi-agent approaches. An example
of this approach is frameworks such as LangChain, or the ChatGPT plugin system which allows interfacing with external Python programs or tools such as Mathematica.

Some previous studies have examined the mathematical reasoning abilities of LLMs, and others have
examined "common-sense" reasoning abilities. Existing datasets for testing reasoning ability include  LogiQA [@doi:10.48550/arXiv.2007.08124] and ReClor [@doi:10.48550/arXiv.2002.04326]. These are aimed at testing reasoning abilities in the
context of natural language understanding. For example, LogiQA contains as instance with paragraph
"_David knows Mr. Zhang's friend Jack, and Jack knows David's friend  Ms. Lin. Everyone of them who knows Jack has a master's degree, and  everyone of them who knows Ms. Lin is from Shanghai._" and question "_Who is from Shanghai and has a masters degree?_".
Liu et al adapted these for evaluation using prompt-based LLMs such as GPT [@doi:10.48550/arXiv.2304.03439].

However, there are no LLM-based benchmarks for testing the kinds of symbolic reasoning abilities
needed for working with ontologies and knowledge bases such as the Gene Ontology (GO) or the
Human Phenotype Ontology (HPO). These abilities are necessary for using ontologies in tasks such
as interpreting high-throughput genomic data or prioritizing variants in genetic diseases. Existing
OWL reasoning benchmarks such as OWL2Bench and LUBM are designed to test the performance at scale rather
than accuracy, since accuracy can generally be proven based on the formal semantics of ontology languages.

Here we describe the creation of a set of benchmarks that can be used to test the reasoning abilities
of LLMs against the commonly used EL++ subset of the Description Logic (DL) language, OWL (Web Ontology Language).
We created a reasoner agent ReasonerGPT that uses an in-context approach to solving reasoning problems, and
we evaluated this against our benchmarks.

### Contributions

We make the following contributions:

- We have curated a collection of benchmarks for testing ontological reasoning ability
- We created a GPT-based reasoning agent that is intended to perform ontological reason using
  a few-shot learning approach, using instruction prompting and in-context examples.
- We have evaluated the reasoning ability of GPT-3.5-turbo and GPT-4 on these benchmarks