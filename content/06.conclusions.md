## Conclusions

We created a series of benchmarks for evaluating the performance of LLMs on ontological reasoning
tasks in the EL++ subset.

We have demonstrated that LLMs are capable of simulating reasoning for certain simple tasks, 
with GPT-4 outperforming the smaller GPT-3.5. However, even
when they perform well, they do not perform as well as standard symbolic reasoners which are guaranteed
to be sound and complete. This indicates that single-pass LLM based approaches on knowledge-oriented
tasks may hide incomplete results, and that hybrid neuro-symbolic approaches are likely to yield
better performance on complex real-world knowledge-oriented tasks.
